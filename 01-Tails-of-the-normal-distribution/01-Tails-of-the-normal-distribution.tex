\documentclass{article}

%%% core packages %%% 

\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage{natbib}
\RequirePackage{hyperref}
\RequirePackage{booktabs, array}
\RequirePackage{lipsum}
\RequirePackage{graphicx}
\RequirePackage{xcolor}
\RequirePackage{subcaption}
\RequirePackage{multirow}
\RequirePackage{float}
\RequirePackage{mdframed}

%%% Referencing %%%

\bibliographystyle{agsm}

%%% page setup %%%

\RequirePackage[utf8]{inputenc}
\RequirePackage{setspace}
\RequirePackage[a4paper,
    left=1.3in,
    right=1in]{geometry}
    
\renewcommand{\baselinestretch}{1.4}


\providecommand{\keywords}[1]{\textbf{Keywords:} #1}


%%% custom maths %%%

\RequirePackage{stackengine}

\newcommand\cleq{\mathrel{\stackunder{$<$}{$\sim$}}}
\newcommand\cgeq{\mathrel{\stackunder{$>$}{$\sim$}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\indep}{\perp \!\!\! \perp}


%%% theorems, lemmas, etc %%%

\definecolor{defcolor}{rgb}{1,0.8,0.6} % Desert
\definecolor{defframe}{rgb}{1,0.5,0.3} % Dark desert
\definecolor{thmcolor}{rgb}{0.8,0.8,1} % Light purple
\definecolor{thmframe}{rgb}{0.4,0.4,0.6} % dark purple

\newtheorem{definition}{Definition}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{proposition}{Proposition}[section]

\newtheorem{example}{Example}[section]
\surroundwithmdframed[
    backgroundcolor=thmcolor,
    linecolor=thmframe,
    linewidth=1.2pt
]{example}

\newtheorem{theorem}{Theorem}[section]
\surroundwithmdframed[
    backgroundcolor=thmcolor,
    linecolor=defframe,
    linewidth=1.2pt
]{theorem}

\newtheorem{lemma}{Lemma}[section]
\surroundwithmdframed[
    backgroundcolor=thmcolor,
    linecolor=defframe,
    linewidth=1.2pt
]{lemma}

\newtheorem{corollary}{Corollary}[section]
\surroundwithmdframed[
    backgroundcolor=thmcolor,
    linecolor=defframe,
    linewidth=1.2pt
]{corollary}

\title{Chapter 1: \emph{tails of the normal distribution}}

\author{Pietro Maria Sparago and Shakeel Gavioli-Akilagun}

\date{\small Last updated: May 29 2024}

\begin{document}

\maketitle

\section{Introduction}

\section{Tails of the normal distribution}

\begin{theorem}  
\label{th1}	
Let $X \sim \mathcal{N}(0,1)$. Define $\overline{\Phi}(x):=P(X>x),x\geq 0$. Then
\begin{equation}
\frac{1}{2}e^{-x\sqrt{2/\pi}-x^2/2}\leq \overline{\Phi}(x)\leq \min\bigg(\frac{1}{2}e^{-x^2/2},\frac{1}{x}\phi(x)\bigg),\,x\geq 0
\end{equation}
\end{theorem}
\begin{proof} We have, by continuity and Corollary \ref{corol_th1}:
\begin{equation}
2\bar{\Phi}(x)=\lim_{y\downarrow 0}\frac{\overline{\Phi}(x+y)}{\overline{\Phi}(y)}
\begin{cases}
\leq \lim_{y\downarrow 0}e^{-xy-x^2/2}=e^{-x^2/2}\\
\geq \lim_{y\downarrow 0}e^{-\rho(y)x-x^2/2}=e^{-x\sqrt{2/\pi}-x^2/2}
\end{cases}
\end{equation}
We conclude for the upper bound with Lemma \ref{lemma: gauss tail bound}.
\end{proof}

\begin{remark} Note that Proposition \ref{th1} yields a stronger version of the infimum Chernoff bound which is found with
$$\begin{aligned}\overline{\Phi}(x)\leq \inf_{\lambda \geq 0}e^{-\lambda x}e^{\lambda^2/2}=e^{-x^2/2},\,x\geq 0
\end{aligned}$$
where the second equality follows from the quadratic minimization condition $\frac{d}{d\lambda}(-\lambda x+\lambda^2/2)=0$ which yields $\lambda=x$.
\end{remark}

\subsection{Lemmas for the proof of Theorem \ref{th1} - mostly from \cite{pollard2002user}; Appendix D}

\begin{lemma} 	
Let $\phi(x):=(2\pi)^{-1/2}e^{-x^2/2}$. Then:
\begin{equation}
\bigg(\frac{1}{x}-\frac{1}{x^3}\bigg)\phi(x)<\overline{\Phi}(x)<\frac{1}{x}\phi(x),\,x>0
\end{equation}
\label{lemma: gauss tail bound}
\end{lemma}
\begin{proof} We have $\overline{\Phi}(x)=\int_x^\infty \phi(t)dt$. Note that for all $t>0$ the functions $1-3/t^4<1$ and $1+1/t^2>1$ so $(1-3/t^4)\phi(t)<\phi(t)<(1+1/t^2)\phi(t)$. We thus integrate from $x>0$ to infinity and obtain the claimed bounds.
\end{proof}
It follows immediately that for the function $\rho(x):=\phi(x)/\overline{\Phi}(x),\,x>0$ we have the bounds
$$x<\rho(x)<\frac{x^3}{x^2-1},\,x>1$$

\begin{theorem}
	\label{rhofunction}
	$\rho$ is increasing, $\rho(-\infty)=0$ and $\rho(0)=\sqrt{2/\pi}$. The function $\rho(x)-x$ decreases to zero as $x$ tends to infinity. The function $\ln \rho(x)$ is concave and $\ln \rho(x+\delta)<\ln \rho (x)+(\rho(x)-x)\delta$ for $x \in \mathbb{R}$ and $\delta >0$.
\end{theorem}
\begin{proof}
	We have
	$$\begin{aligned}\frac{1}{\rho(x)}=\frac{\overline{\Phi}(x)}{\phi(x)}&=\int_x^\infty\frac{\phi(t)}{\phi(x)}dt\\
	&\stackrel{z=t-x}=\int_0^\infty\frac{\phi(z+x)}{\phi(x)}dz\\
	&=\int_0^\infty e^{-zx}e^{-z^2/2}dz
	\end{aligned}$$
	That is, the Laplace transform of the finite measure $\mu(dz) := e^{-z^2/2}dz,z>0$. From this it follows that $\rho(0)=\sqrt{2/\pi}$ and that $\rho(-\infty)=0$, since by monotone convergence $\lim_{x\to -\infty}\frac{1}{\rho(x)}=\infty$. For $x\geq 0$ we have $ z e^{-zx}\leq z,z>0$ and $z \in L^1(d\mu)$. So by differentiation-under-integral lemma we have
	$$\begin{aligned}\rho(x)\bigg(\frac{d}{dx}\frac{1}{\rho(x)}\bigg)&=\rho(x)\bigg(-\int_0^\infty ze^{-zx}\mu(dz)\bigg)<0\\
	\rho(x)\bigg(\frac{d^2}{dx^2}\frac{1}{\rho(x)}\bigg)&=\rho(x)\int_0^\infty z^2 e^{-zx}\mu(dz)>0\\
	\end{aligned}$$
	The function $1/\rho$ is decreasing because $\frac{d}{dx}\frac{1}{\rho(x)}<0$, so that $\rho$ is increasing. Now, we have:
	$$\begin{aligned}\ln \rho(x)=\ln(\phi(x))-\ln(\overline{\Phi}(x))=-\ln(\sqrt{2\pi})-\frac{x^2}{2}-\ln(\overline{\Phi}(x))
	\end{aligned}$$
	Therefore 
	$$\frac{d}{dx}\ln \rho(x)=-x-\frac{1}{\overline{\Phi}(x)}\frac{d}{dx}\overline{\Phi}(x)=\frac{\phi(x)}{\overline{\Phi}(x)}-x=\rho(x)-x$$
	But we also have $\frac{d}{dx}(-\ln \rho(x))=\frac{d}{dx}\ln (1/\rho(x))$ so
	$$\begin{aligned}\frac{d}{dx}(\rho(x)-x)&=-\frac{d^2}{dx^2}\ln\frac{1}{\rho(x)}\\
	&=-\frac{d}{dx}\bigg(\rho(x)\bigg(\frac{d}{dx}\frac{1}{\rho(x)}\bigg)\bigg)\\
	&=-\bigg(\rho(x)\bigg(\frac{d^2}{dx^2}\frac{1}{\rho(x)}\bigg)+\bigg(\bigg(\frac{d}{dx}\rho(x)\bigg)\bigg(\frac{d}{dx}\frac{1}{\rho(x)}\bigg)\bigg)
	\end{aligned}$$
	and
	$$\frac{d}{dx}\frac{1}{\rho(x)}=-\frac{d}{dx}\rho(x)\frac{1}{\rho(x)^2}\implies \frac{d}{dx}\rho(x)=-\rho(x)^2\bigg(\frac{d}{dx}\frac{1}{\rho(x)}\bigg)$$
	Therefore 
	$$\frac{d}{dx}(\rho(x)-x)=-\bigg(\rho(x)\bigg(\frac{d^2}{dx^2}\frac{1}{\rho(x)}\bigg)-\rho(x)^2\bigg(\frac{d}{dx}\frac{1}{\rho(x)}\bigg)^2\bigg)$$
	The term in the parenthesis in the variance of the measure $\nu_x(dz):=\rho(x)e^{-zx}\mu(dz)$, which is strictly positive. In fact for any $x\geq 0$, $\nu_x(dz)$ is a probability measure:
	$$\int_0^\infty\nu_x(dz)=\rho(x)\int_0^\infty e^{-zx-z^2/2}dz=\rho(x)\int_x^\infty\frac{\phi(t)}{\phi(x)}dt=1$$
	Thus, $\rho(x)-x$ is decreasing because its first derivative is strictly negative. Now since
	$$\begin{aligned}
	\frac{d^2}{dx^2}\ln \rho(x)&=\frac{d}{dx}(\rho(x)-x)<0\\
	\end{aligned}$$
	then the function $\ln \rho(x)$ is thus concave. To see that $\rho(x)-x$ vanishes as $x\to \infty$:
	$$0<\rho(x)-x<\frac{x}{x^2-1}\stackrel{x\to \infty}{\to} 0$$
	It remains to show the last claim. For $\delta >0$ and some $x^*\in (x,x+\delta)$, since $\rho(x)-x$ is decreasing:
	$$\ln\frac{\rho(x+\delta)}{\rho(x)}=\delta \frac{\ln \rho(x+\delta)-\ln \rho(x)}{\delta}\stackrel{\textrm{MVT}}{=}\delta(\rho(x^*)-x^*)<\delta(\rho(x)-x)$$
	and we conclude.
\end{proof}

\begin{corollary}
\label{corol_th1}
For $x\geq 0$ and $\delta>0$:
$$e^{-\rho(x)\delta -\delta^2/2}\leq \frac{\overline{\Phi}(x+\delta)}{\overline{\Phi}(x)}\leq e^{-x\delta -\delta^2/2}$$
\end{corollary}
\begin{proof} We have
$$\begin{aligned}\overline{\Phi}(x+\delta)&=\int_{x}^\infty\phi(z+\delta)dz\\
&=\frac{e^{-\delta^2/2}}{\sqrt{2\pi}}\int_{x}^\infty e^{-z^2/2-z\delta}dz\\
&\stackrel{e^{-z\delta}\leq e^{-x\delta},\forall z\geq x}{\leq }\frac{e^{-\delta^2/2-x\delta}}{\sqrt{2\pi}}\int_{x}^\infty e^{-z^2/2}dz\\
&=e^{-\delta^2/2-x\delta}\overline{\Phi}(x)
\end{aligned}$$
By Theorem \ref{rhofunction} we have $
\ln\frac{\rho(x+\delta)}{\rho(x)}< (\rho(x)-x)\delta$. We also have
$$\begin{aligned}\frac{\overline{\Phi}(x+\delta)}{\overline{\Phi}(x)}&=\frac{\overline{\Phi}(x+\delta)}{\phi(x+\delta)}\frac{\phi(x)}{\overline{\Phi}(x)}\frac{\phi(x+\delta)}{\phi(x)}\\
&=\frac{\rho(x+\delta)}{\rho(x)}\frac{\phi(x+\delta)}{\phi(x)}\\
&=e^{-\ln\frac{\rho(x+\delta)}{\rho(x)}}e^{-\delta^2/2-x\delta}\\
&\geq e^{-\rho(x)\delta -\delta^2/2}
\end{aligned}$$
and we conclude.
\end{proof}

\section{Laws of Iterated Logarithm}

\subsection{Motivation}

Let $X_1, X_2, \dots$ be a sequence of independently and identically distributed random variables with mean zero and variance one, and let $S_n = X_1 + \dots + X_n$ for $n \geq 1$ and $S_0 = 0$ be the associated partial sum process. The strong law of large numbers states that scaled by $n^{-1}$ the partial sum process converges almost surely to the mean of the $X$'s, in this case zero: 
\begin{equation}
	n^{-1} S_n \xrightarrow{a.s.} 0 \text{ as } n \rightarrow \infty. 
	\label{equation: strong LLN}
\end{equation}
Whereas the central limit theorem states that scaled by $n^{-1/2}$ the partial sum process converges in distribution to a standard Gaussian: 
\begin{equation}
	n^{-1/2} S_n \xrightarrow{d} \mathcal{N} \left ( 0, 1 \right ) \text{ as } n \rightarrow \infty. 
	\label{equation: CLT}
\end{equation}
It is often of interest to quantify the fluctuations of $S_n$. The law of iterated logarithm provides such a result, and states that almost surely the fluctuations will be no larger than $\sqrt{2 n \log \log (n)}$. Comparing to the scaling factors in (\ref{equation: strong LLN}) and (\ref{equation: CLT}) the law of iterated logarithm can be understood as operating \emph{between} the central limit theorem and the law of large numbers. 

\subsection{A law of iterated logarithm for the Wiener process}

We present a law of iterated logarithm for the Wiener process. In section \ref{section: laws of iterated logarithm for discrete random walks} this result will be used as a building block for proving laws of iterated logarithms for more general random walks. 

\begin{theorem}
Let $\left (B_t \right )_{t \geq 0}$ be a Wiener process, then $\limsup\limits_{n \rightarrow \infty} B_n / \sqrt{2 n \log \log (n)} = 1 $, $\mathbb{P}$-almost surely. 
\label{theorem: BM LIL}
\end{theorem}
We present the proof of Theorem \ref{theorem: BM LIL} which can be found in section 8.5 of \cite{durrett2019probability}. The proof relies on the the reflection principle for Wiener processes, which is stated below. 
\begin{lemma}
Let $\left (B_t \right )_{t \geq 0}$ be a Wiener process, putting $T_a = \inf \left \{ t > 0 \mid B_t = a \right \}$ with $a > 0$ it holds that $\mathbb{P} \left ( T_a < t \right ) = 2 \mathbb{P} \left ( B_t \geq a \right )$. 
\end{lemma}
\begin{proof}
If $B$ hits $a$ at some time $s < t$ then $B_t - B_{T_a}$ will be independent of $B_{T_a^-}$. The symmetry of the normal distribution along with the fact that $\mathbb{P}_a \left ( B_u = a \right ) = 0 $ for $u > 0$ then imply that
\begin{equation}
\mathbb{P} \left ( T_a < t, B_t > a \right ) = \frac{1}{2} \mathbb{P} \left ( T_a < t \right ). 
\label{equation: hitting prob}
\end{equation}
\label{lemma: reflection principle}
\end{proof} 

Above we have written $\mathbb{P}_x$ for the probability conditioned on the fact that the Wiener process starts from $x$. Strictly speaking (\ref{equation: hitting prob}) needs to be proved formally, and a proof can be found on page 323 of \cite{durrett2019probability}. Using the fact that $\left \{ B_t > a \right \} \subset \left \{ T_a < t \right \} $ and rearranging (\ref{equation: hitting prob}) gives the desired result. With this result in place we are now able to prove Theorem \ref{theorem: BM LIL}.

\begin{proof}
Using Lemma \ref{lemma: reflection principle} we have that
\begin{equation}
\mathbb{P} \left ( \sup_{s \in [0,1]} B_s > x \right ) = \mathbb{P} \left ( T_x \leq 1 \right ) = 2 \bar{\Phi} \left ( x \right ) 
\end{equation}
Lemma \ref{lemma: gauss tail bound} implies that $\bar{\Phi} (x) \sim \frac{1}{\sqrt{2 \pi}} x^{-1} e^{-x^2/2}$ as $x \rightarrow \infty$, therefore combing this with the scaling property of Wiener processes we have that for any increasing function $f$
\begin{equation}
\mathbb{P} \left ( B_n > \left ( n f(n) \right ) ^ {1/2} \right ) \sim \frac{1}{\sqrt{2\pi}} f(n)^{-1/2} \exp \left ( - f(n) / 2 \right ). 
\label{equation: f(n) prob}
\end{equation}
Using (\ref{equation: f(n) prob}) we therefore have for any $\varepsilon > 0$ that 
\begin{equation}
\sum_{n = 1}^{\infty} \mathbb{P} \left ( B_n > \left ( n f(n) \right ) ^ {1/2} \right ) \left\{\begin{matrix}
< \infty & \text{ when } f(n) = (2 + \varepsilon ) \log (n) \\
= \infty & \text{ when } f(n) = (2 - \varepsilon ) \log (n)\\
\end{matrix}\right. .
\end{equation}
Hence, the Borel-Cantelli lemma gives that $\mathbb{P}$-almost surely $\limsup\limits{n \rightarrow \infty} B_{n} / \sqrt{2n \log (n)} \leq 1$. To replace the $\log (n)$ term with $\log \log (n)$ we need to look at an exponentially growing sequence. Let $t_n = \alpha^n$ for some $\alpha > 1$. Then
\begin{align}
\mathbb{P} \left ( \sup_{s \in [t_n, t_{n+1}]} B_s > \left ( t_n f (t_n) \right ) ^ {1/2} \right ) & \leq \mathbb{P} \left ( \sup_{s \in [t_n, t_{n+1}]} B_s / \sqrt{t_{n+1}} > \left ( \frac{f (t_n)}{\alpha} \right ) ^ {1/2} \right ) \\ 
& \leq \frac{1}{\sqrt{2\pi}} \left ( \frac{f(t_n)}{\alpha} \right ) ^ {-1/2} \exp \left ( - f (t_n) / 2 \alpha \right ).
\end{align}
If $f(n) = 2 \alpha^2 \log \log (n)$ then $f(t_n) = \log (n) + \log \log (\alpha)$ and therefor $\exp \left ( - f (t_n) / 2 \alpha \right ) \leq C_\alpha n^{-\alpha}$ for some $C_\alpha > 0$ depending only on $\alpha$. Hence
\begin{equation}
\sum_{n=1}^\infty \mathbb{P} \left ( \sup_{s \in [t_n, t_{n+1}]} B_s > \left ( t_n f (t_n) \right ) \right ) < \infty. 
\end{equation}
Since $t \mapsto \left ( t f(t) \right )^{1/2}$ is increasing and $\alpha$ is arbitrary we obtain that $\mathbb{P}$-almost surely
\begin{equation}
\limsup_{n \rightarrow \infty} B_n / \sqrt{2 \log \log (n)} \leq 1. 
\end{equation} 
To prove a weak inequality in the other direction let $t_n$ be defined as before but this time take $\alpha$ to be large in order to get independence. We put $\beta = t_{n+1} / (t_{n+1} - t_n ) = \alpha / (\alpha - 1)$ and look at the exceedance probability of the increments: 
\begin{equation}
\mathbb{P} \left ( B_{t_{n+1}} - B_{t_n} > \left ( t_{n+1} f (t_{n+1}) \right ) ^ {1/2} \right ) = \mathbb{P} \left ( B_1 > \left ( \beta f (t_{n+1}) \right ) ^ {1/2} \right ). 
\label{equation: increments exc}
\end{equation}
Again using Lemma \ref{lemma: gauss tail bound} (\ref{equation: increments exc}) will be no smaller than $\frac{1}{2\sqrt{2\pi}} \left ( \beta f (t_{n+1}) \right ) ^ {-1/2} \exp \left ( - \beta f \left ( t_{n+1} \right ) / 2 \right )$. Therefore, for $n$ sufficiently large and putting $f(n) = (2/\beta^2) \log \log (n)$ we obtain that $\exp \left ( - \beta f \left ( t_{n+1} \right ) / 2 \right )$ is no smaller than $C_\alpha n^{-1/\beta}$. Therefore: 
\begin{equation}
\sum_{n=0}^\infty \mathbb{P} \left ( B_{t_{n+1}} - B_{t_n} > \left ( t_{n+1} f (t_{n+1}) \right ) ^ {1/2} \right ) = \infty. 
\end{equation}
Since the events in question are independent we obtain by Borel-Cantelli that infinity often
\begin{equation}
B_{t_{n+1}} - B_{t_n} > \sqrt{\left ( 2 / \beta^2 \right ) t_{n+1} \log \log (t_{n+1})}. 
\end{equation}
Plugging in the definition of $\beta$, and using the facts that $t_n = t_{n+1} / \alpha$ along with the fact that $\log \log (n)$ is monotone increasing we obtain that 
\begin{equation}
\limsup_{n \rightarrow \infty} B_{t_{n+1}} / \sqrt{2 t_{n+1} \log \log (t_{n+1})} \geq \frac{\alpha-1}{\alpha} - \frac{1}{\sqrt{\alpha}}. 
\end{equation}
Finally, letting $\alpha \rightarrow \infty$ proves the result. 
\end{proof}

\subsection{Laws of iterated logarithm for discrete random walks} \label{section: laws of iterated logarithm for discrete random walks}

We now present a law of iterated logarithms for discrete random walks with finite second moment. The main theoretical device will be the embedding of random walks from independent random variables with bounded moments into Wiener processes. Such a result is given below. 

\begin{lemma}
Let $X_1, X_2, \dots$ be independently distributed with mean zero, variance one, and common marginal distribution $F$. Putting $S_n = X_1 + \dots + X_n$ for $n \geq 1$ there is a sequence of stopping times $T_0, T_1, \dots$ with $T_0 = $ such that $S_n \stackrel{d}{=} B_{T_n}$ and moreover the increments $T_n - T_{n-1}$ are independently distributed. 
\label{lemma: random walk coupling}
\end{lemma}

The lemma is not hard to prove, and its proof can be found for for example in Theorem 8.1.2 of \cite{durrett2019probability}. However, we defer the proof to Chapter 10 where couplings of empirical processes and Wiener process will be discussed in detail. Combining Theorem \ref{theorem: BM LIL} with Lemma \ref{lemma: random walk coupling} readily yields the following result. 

\begin{theorem}
If $X_1, X_2, \dots$ are independently distributed with mean zero, variance one, and common distribution function $F$ then $\mathbb{P}$-almost surely $\limsup \limits_{n \rightarrow \infty} S_n / \sqrt{2 n \log \log (n)} = 1$. 
\label{theorem: LIL random walk}
\end{theorem}

Another approach to proving the theorem is given in Chapter 11 of \cite{pollard2002user}

\subsection{Distributional convergence}

In can be of interest to understand the distribution of the maximal fluctuations in a standardised random walk. The following result can be found for example in \cite{kabluchko2007extreme}. 

\begin{theorem}
Let $\left (B_t \right )_{t \geq 0}$ be a Wiener process and for $n > 1$ put $M_n = \sup_{s \in [0,n]} s^{-1/2} B_s$. Then for every fixed $z \in \mathbb{R}$ it holds that $\mathbb{P} \left ( M_n \leq a_n + z b_n \right ) \rightarrow \exp (e^{-z})$ as $n \rightarrow \infty$, where in particular the normalizing constants are given by
\begin{align}
& a_n = \sqrt{2 \log \log (n)} + \frac{1/2 \log \log \log (n) - \log \left ( 2 \sqrt{\pi} \right )}{\sqrt{2 \log \log (n)}} \\ 
& b_n = \frac{1}{\sqrt{2 \log \log (n)}}.
\end{align}
\label{theorem: distributional LIL for Wiener process}
\end{theorem}

Using the same approach as in Section \ref{section: laws of iterated logarithm for discrete random walks} \cite{darling1956limit} used the above theorem to prove the following result. 

\begin{theorem}
Let $X_1, X_2, \dots$ be independently distributed with marginal $\mathcal{N} (0,1)$ distribution, put $S_n = X_1 + \dots + X_n$, and put $M_n = \max_{s \in \left \{ 1, \dots, n \right \}} s^{-1/2} S_s$. Then for any fixed $z \in \mathbb{R}$ it holds that $\mathbb{P} \left ( M_n \leq a_n + z b_n \right ) \rightarrow \exp (e^{-z})$ as $n \rightarrow \infty$, where the normalizing constants are the same as those given in Theorem \ref{theorem: distributional LIL for Wiener process}. 
\label{theorem: distributional LIL for random walk}
\end{theorem}

\subsection{Non-asymptotic laws of iterated logarithm}

In applications it often useful to have a finite sample counterpart to Theorems \ref{theorem: BM LIL} and \ref{theorem: LIL random walk}. To this end \cite{balsubramani2014sharp} proves the following result. 

\begin{theorem}
Let $X_1, X_2, \dots$ be mutually independent Rademacher random variables and put $S_n = X_1 + \dots + X_n$. There is an absolute constant $C$, where it suffices to take $C = 137$, such that for any $\delta \in (0,1)$ and any $n > C \log (4 / \delta)$ it holds with probability at least $1 - \delta$ that
\begin{equation}
\left | S_n \right | \leq \sqrt{3n \left( 2 \log \log \left ( \frac{5}{2} n \right ) + \log \left ( \frac{2}{\delta} \right ) \right )} \vee 1. 
\label{eqation: finite sample LIL}
\end{equation}
\label{theorem: finite sample LIL}
\end{theorem}

The leading constant $\sqrt{6}$ in (\ref{eqation: finite sample LIL}) does not seem optimal, as in fact Theorem \ref{theorem: BM LIL} suggests the constant should be $\sqrt{2}$. Nevertheless, the result captures the first order asymptotics of the maximum fluctuations in the random walk, and makes explicit the trade off between $\delta$ and $n$. \cite{balsubramani2014sharp} extend the result to martingales with uniformly bounded increments. Using the strong approximation results in \cite{komlos1975approximation} it should be possible to extend Theorem \ref{theorem: finite sample LIL} to other discrete random walks not necessarily having bounded increments. 

\newpage

\bibliographystyle{ksfh_nat}
\bibliography{../bibliography/ref}

\end{document}