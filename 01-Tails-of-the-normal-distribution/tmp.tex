\documentclass{article}

%%% core packages %%% 

\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage{natbib}
\RequirePackage{hyperref}
\RequirePackage{booktabs, array}
\RequirePackage{lipsum}
\RequirePackage{graphicx}
\RequirePackage{xcolor}
\RequirePackage{subcaption}
\RequirePackage{multirow}
\RequirePackage{float}
\RequirePackage{mdframed}

%%% Referencing %%%

\bibliographystyle{agsm}

%%% page setup %%%

\RequirePackage[utf8]{inputenc}
\RequirePackage{setspace}
\RequirePackage[a4paper,
    left=1.3in,
    right=1in]{geometry}
    
\renewcommand{\baselinestretch}{1.4}


\providecommand{\keywords}[1]{\textbf{Keywords:} #1}


%%% custom maths %%%

\RequirePackage{stackengine}

\newcommand\cleq{\mathrel{\stackunder{$<$}{$\sim$}}}
\newcommand\cgeq{\mathrel{\stackunder{$>$}{$\sim$}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\indep}{\perp \!\!\! \perp}


%%% theorems, lemmas, etc %%%

\definecolor{defcolor}{rgb}{1,0.8,0.6} % Desert
\definecolor{defframe}{rgb}{1,0.5,0.3} % Dark desert
\definecolor{thmcolor}{rgb}{0.8,0.8,1} % Light purple
\definecolor{thmframe}{rgb}{0.4,0.4,0.6} % dark purple

\newtheorem{definition}{Definition}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{proposition}{Proposition}[section]

\newtheorem{example}{Example}[section]
\surroundwithmdframed[
    backgroundcolor=thmcolor,
    linecolor=thmframe,
    linewidth=1.2pt
]{example}

\newtheorem{theorem}{Theorem}[section]
\surroundwithmdframed[
    backgroundcolor=thmcolor,
    linecolor=defframe,
    linewidth=1.2pt
]{theorem}

\newtheorem{lemma}{Lemma}[section]
\surroundwithmdframed[
    backgroundcolor=thmcolor,
    linecolor=defframe,
    linewidth=1.2pt
]{lemma}

\newtheorem{corollary}{Corollary}[section]
\surroundwithmdframed[
    backgroundcolor=thmcolor,
    linecolor=defframe,
    linewidth=1.2pt
]{corollary}

\title{Chapter 1: \emph{tails of the normal distribution}}

\author{Pietro Maria Sparago and Shakeel Gavioli-Akilagun}

\date{\small Last updated: May 29 2024}

\begin{document}

\maketitle

\section{Introduction}

\section{Tails of the normal distribution}

\begin{theorem}  
\label{th1}	
Let $X \sim \mathcal{N}(0,1)$. Define $\overline{\Phi}(x):=P(X>x),x\geq 0$. Then
\begin{equation}
\frac{1}{2}e^{-x\sqrt{2/\pi}-x^2/2}\leq \overline{\Phi}(x)\leq \min\bigg(\frac{1}{2}e^{-x^2/2},\frac{1}{x}\phi(x)\bigg),\,x\geq 0
\end{equation}
\end{theorem}
\begin{proof} We have, by continuity and Corollary \ref{corol_th1}:
\begin{equation}
2\bar{\Phi}(x)=\lim_{y\downarrow 0}\frac{\overline{\Phi}(x+y)}{\overline{\Phi}(y)}
\begin{cases}
\leq \lim_{y\downarrow 0}e^{-xy-x^2/2}=e^{-x^2/2}\\
\geq \lim_{y\downarrow 0}e^{-\rho(y)x-x^2/2}=e^{-x\sqrt{2/\pi}-x^2/2}
\end{cases}
\end{equation}
We conclude for the upper bound with Lemma \ref{lemma1_th1}.
\end{proof}

\begin{remark} Note that Proposition \ref{th1} yields a stronger version of the infimum Chernoff bound which is found with
$$\begin{aligned}\overline{\Phi}(x)\leq \inf_{\lambda \geq 0}e^{-\lambda x}e^{\lambda^2/2}=e^{-x^2/2},\,x\geq 0
\end{aligned}$$
where the second equality follows from the quadratic minimization condition $\frac{d}{d\lambda}(-\lambda x+\lambda^2/2)=0$ which yields $\lambda=x$.
\end{remark}

\subsection{Lemmas for the proof of Theorem \ref{th1} - mostly from \cite{pollard2002user}; Appendix D}

\begin{lemma} 
\label{lemma1_th1}	
Let $\phi(x):=(2\pi)^{-1/2}e^{-x^2/2}$. Then:
\begin{equation}
\bigg(\frac{1}{x}-\frac{1}{x^3}\bigg)\phi(x)<\overline{\Phi}(x)<\frac{1}{x}\phi(x),\,x>0
\end{equation}
\end{lemma}
\begin{proof} We have $\overline{\Phi}(x)=\int_x^\infty \phi(t)dt$. Note that for all $t>0$ the functions $1-3/t^4<1$ and $1+1/t^2>1$ so $(1-3/t^4)\phi(t)<\phi(t)<(1+1/t^2)\phi(t)$. We thus integrate from $x>0$ to infinity and obtain the claimed bounds.
\end{proof}
It follows immediately that for the function $\rho(x):=\phi(x)/\overline{\Phi}(x),\,x>0$ we have the bounds
$$x<\rho(x)<\frac{x^3}{x^2-1},\,x>1$$

\begin{theorem}
	\label{rhofunction}
	$\rho$ is increasing, $\rho(-\infty)=0$ and $\rho(0)=\sqrt{2/\pi}$. The function $\rho(x)-x$ decreases to zero as $x$ tends to infinity. The function $\ln \rho(x)$ is concave and $\ln \rho(x+\delta)<\ln \rho (x)+(\rho(x)-x)\delta$ for $x \in \mathbb{R}$ and $\delta >0$.
\end{theorem}
\begin{proof}
	We have
	$$\begin{aligned}\frac{1}{\rho(x)}=\frac{\overline{\Phi}(x)}{\phi(x)}&=\int_x^\infty\frac{\phi(t)}{\phi(x)}dt\\
	&\stackrel{z=t-x}=\int_0^\infty\frac{\phi(z+x)}{\phi(x)}dz\\
	&=\int_0^\infty e^{-zx}e^{-z^2/2}dz
	\end{aligned}$$
	That is, the Laplace transform of the finite measure $\mu(dz) := e^{-z^2/2}dz,z>0$. From this it follows that $\rho(0)=\sqrt{2/\pi}$ and that $\rho(-\infty)=0$, since by monotone convergence $\lim_{x\to -\infty}\frac{1}{\rho(x)}=\infty$. For $x\geq 0$ we have $ z e^{-zx}\leq z,z>0$ and $z \in L^1(d\mu)$. So by differentiation-under-integral lemma we have
	$$\begin{aligned}\rho(x)\bigg(\frac{d}{dx}\frac{1}{\rho(x)}\bigg)&=\rho(x)\bigg(-\int_0^\infty ze^{-zx}\mu(dz)\bigg)<0\\
	\rho(x)\bigg(\frac{d^2}{dx^2}\frac{1}{\rho(x)}\bigg)&=\rho(x)\int_0^\infty z^2 e^{-zx}\mu(dz)>0\\
	\end{aligned}$$
	The function $1/\rho$ is decreasing because $\frac{d}{dx}\frac{1}{\rho(x)}<0$, so that $\rho$ is increasing. Now, we have:
	$$\begin{aligned}\ln \rho(x)=\ln(\phi(x))-\ln(\overline{\Phi}(x))=-\ln(\sqrt{2\pi})-\frac{x^2}{2}-\ln(\overline{\Phi}(x))
	\end{aligned}$$
	Therefore 
	$$\frac{d}{dx}\ln \rho(x)=-x-\frac{1}{\overline{\Phi}(x)}\frac{d}{dx}\overline{\Phi}(x)=\frac{\phi(x)}{\overline{\Phi}(x)}-x=\rho(x)-x$$
	But we also have $\frac{d}{dx}(-\ln \rho(x))=\frac{d}{dx}\ln (1/\rho(x))$ so
	$$\begin{aligned}\frac{d}{dx}(\rho(x)-x)&=-\frac{d^2}{dx^2}\ln\frac{1}{\rho(x)}\\
	&=-\frac{d}{dx}\bigg(\rho(x)\bigg(\frac{d}{dx}\frac{1}{\rho(x)}\bigg)\bigg)\\
	&=-\bigg(\rho(x)\bigg(\frac{d^2}{dx^2}\frac{1}{\rho(x)}\bigg)+\bigg(\bigg(\frac{d}{dx}\rho(x)\bigg)\bigg(\frac{d}{dx}\frac{1}{\rho(x)}\bigg)\bigg)
	\end{aligned}$$
	and
	$$\frac{d}{dx}\frac{1}{\rho(x)}=-\frac{d}{dx}\rho(x)\frac{1}{\rho(x)^2}\implies \frac{d}{dx}\rho(x)=-\rho(x)^2\bigg(\frac{d}{dx}\frac{1}{\rho(x)}\bigg)$$
	Therefore 
	$$\frac{d}{dx}(\rho(x)-x)=-\bigg(\rho(x)\bigg(\frac{d^2}{dx^2}\frac{1}{\rho(x)}\bigg)-\rho(x)^2\bigg(\frac{d}{dx}\frac{1}{\rho(x)}\bigg)^2\bigg)$$
	The term in the parenthesis in the variance of the measure $\nu_x(dz):=\rho(x)e^{-zx}\mu(dz)$, which is strictly positive. In fact for any $x\geq 0$, $\nu_x(dz)$ is a probability measure:
	$$\int_0^\infty\nu_x(dz)=\rho(x)\int_0^\infty e^{-zx-z^2/2}dz=\rho(x)\int_x^\infty\frac{\phi(t)}{\phi(x)}dt=1$$
	Thus, $\rho(x)-x$ is decreasing because its first derivative is strictly negative. Now since
	$$\begin{aligned}
	\frac{d^2}{dx^2}\ln \rho(x)&=\frac{d}{dx}(\rho(x)-x)<0\\
	\end{aligned}$$
	then the function $\ln \rho(x)$ is thus concave. To see that $\rho(x)-x$ vanishes as $x\to \infty$:
	$$0<\rho(x)-x<\frac{x}{x^2-1}\stackrel{x\to \infty}{\to} 0$$
	It remains to show the last claim. For $\delta >0$ and some $x^*\in (x,x+\delta)$, since $\rho(x)-x$ is decreasing:
	$$\ln\frac{\rho(x+\delta)}{\rho(x)}=\delta \frac{\ln \rho(x+\delta)-\ln \rho(x)}{\delta}\stackrel{\textrm{MVT}}{=}\delta(\rho(x^*)-x^*)<\delta(\rho(x)-x)$$
	and we conclude.
\end{proof}

\begin{corollary}
\label{corol_th1}
For $x\geq 0$ and $\delta>0$:
$$e^{-\rho(x)\delta -\delta^2/2}\leq \frac{\overline{\Phi}(x+\delta)}{\overline{\Phi}(x)}\leq e^{-x\delta -\delta^2/2}$$
\end{corollary}
\begin{proof} We have
$$\begin{aligned}\overline{\Phi}(x+\delta)&=\int_{x}^\infty\phi(z+\delta)dz\\
&=\frac{e^{-\delta^2/2}}{\sqrt{2\pi}}\int_{x}^\infty e^{-z^2/2-z\delta}dz\\
&\stackrel{e^{-z\delta}\leq e^{-x\delta},\forall z\geq x}{\leq }\frac{e^{-\delta^2/2-x\delta}}{\sqrt{2\pi}}\int_{x}^\infty e^{-z^2/2}dz\\
&=e^{-\delta^2/2-x\delta}\overline{\Phi}(x)
\end{aligned}$$
By Theorem \ref{rhofunction} we have $
\ln\frac{\rho(x+\delta)}{\rho(x)}< (\rho(x)-x)\delta$. We also have
$$\begin{aligned}\frac{\overline{\Phi}(x+\delta)}{\overline{\Phi}(x)}&=\frac{\overline{\Phi}(x+\delta)}{\phi(x+\delta)}\frac{\phi(x)}{\overline{\Phi}(x)}\frac{\phi(x+\delta)}{\phi(x)}\\
&=\frac{\rho(x+\delta)}{\rho(x)}\frac{\phi(x+\delta)}{\phi(x)}\\
&=e^{-\ln\frac{\rho(x+\delta)}{\rho(x)}}e^{-\delta^2/2-x\delta}\\
&\geq e^{-\rho(x)\delta -\delta^2/2}
\end{aligned}$$
and we conclude.
\end{proof}

\section{Laws of Iterated Logarithm}

\subsection{Motivation}

Let $X_1, X_2, \dots$ be a sequence of independently and identically distributed random variables with mean zero and variance one, and let $S_n = X_1 + \dots + X_n$ for $n \geq 1$ and $S_0 = 0$ be the associated partial sum process. The strong law of large numbers states that scaled by $n^{-1}$ the partial sum process converges almost surely to the mean of the $X$'s, in this case zero: 
\begin{equation}
	n^{-1} S_n \xrightarrow{a.s.} 0 \text{ as } n \rightarrow \infty. 
	\label{equation: strong LLN}
\end{equation}
Whereas the central limit theorem states that scaled by $n^{-1/2}$ the partial sum process converges in distribution to a standard Gaussian: 
\begin{equation}
	n^{-1/2} S_n \xrightarrow{d} \mathcal{N} \left ( 0, 1 \right ) \text{ as } n \rightarrow \infty. 
	\label{equation: CLT}
\end{equation}
It is often of interest to quantify the fluctuations of $S_n$. The law of iterated logarithm provides such a result, and states that almost surely the fluctuations will be no larger than $\sqrt{2 n \log \log (n)}$. Comparing to the scaling factors in (\ref{equation: strong LLN}) and (\ref{equation: CLT}) the law of iterated logarithm can be understood as operating \emph{between} the central limit theorem and the law of large numbers. 

\subsection{A law of iterated logarithm for the Wiener process}

We present a law of iterated logarithm for the Wiener process. In section \ref{section: laws of iterated logarithm for discrete random walks} this result will be used as a building block for proving laws of iterated logarithms for more general random walks. 

\begin{theorem}
Let $\left (B_t \right )_{t \geq 0}$ be a Wiener process, then $\mathbb{P}$-almost surely
\begin{equation}
\limsup\limits_{n \rightarrow \infty} \frac{B_n}{\sqrt{2 n \log \log (n)}} = 1 
\end{equation}
\label{theorem: BM LIL}
\end{theorem}
We present the proof of Theorem \ref{theorem: BM LIL} which can be found in section 8.5 of \cite{durrett2019probability}. The proof relies on the the reflection principle for Wiener processes, which is stated below. 
\begin{lemma}
Let $\left (B_t \right )_{t \geq 0}$ be a Wiener process, putting $T_a = \inf \left \{ t > 0 \mid B_t = a \right \}$ with $a > 0$ it holds that $\mathbb{P} \left ( T_a < t \right ) = 2 \mathbb{P} \left ( B_t \geq a \right )$. 
\end{lemma}
\begin{proof}
this is the proof. 
\end{proof}

\subsection{Laws of iterated logarithm for discrete random walks} \label{section: laws of iterated logarithm for discrete random walks}

Another approach to proving the theorem is given in Chapter 11 of \cite{pollard2002user}

\subsection{Distributional convergence}

\subsection{Non-asymptotic laws of iterated logarithm}

\newpage

\bibliographystyle{ksfh_nat}
\bibliography{../bibliography/ref}

\end{document}